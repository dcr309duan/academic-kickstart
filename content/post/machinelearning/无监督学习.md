---
title: "无监督学习"
date: 2019-05-23T11:16:40+08:00
draft: false
math: true
# Tags and categories
# For example, use `tags = []` for no tags, or the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags: ['Machine Learning', 'Unsupervised Learning']
categories: ['Machine Learning']
# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
  # Caption (optional)
  caption: ''

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point: ''
---

# Unsupervised Learning

* Supervised learning: given a labeled training set to fit a hypothesis to it.
* Unsupervised learning: given a unlabeled set to an algorithm and ask the algorithm find some structure in the data.
  * Clustering algorithm

## Clustering

We are given an unlabeled data set and we would like to have an algorithm automatically group the data into coherent subsets or into coherent clusters for us.

### K-Means Algorithm

1. Randomly initialize two (the number of groups you want your data to be grouped to) points, called cluster centroids.

2. Iteration

   1. cluster assignment step

      * assign each point to one of the two groups.

        ![image-20190515144556808](https://markdown-1252040768.cos.ap-beijing.myqcloud.com/2019-05-23-032105.png)

   2. move centroid step

      * Move to average position of every group

Formal illustration:

* Input:
  * $K$ (number of clusters)
  * Training set $\{x^{(1)}, x^{(2)}, \dots, x^{(m)}\}$
  * $x^{(i)} \in R^n$

1. Randomly initialize $K$ cluster centroids $\mu_1, \mu_2, \dots, \mu_K \in R^n$

2. Repeat

   1. for $i=1$ to $m$

      $c^{(i)}:=$ index (from 1 to $K$) of cluster centroid closest ($||x^{(i)}-\mu_k||$) to $x^{(i)}$

   2. for $k=1$ to $K$

      $\mu_k:=$ average (mean) of points assigned to cluster $k$

An K-means application:

![image-20190523154319441](https://markdown-1252040768.cos.ap-beijing.myqcloud.com/2019-05-23-074319.png)

### Optimization Objective

#### Why we need an optimization objective

1. An objective function can help us to determine that the algorithm work correctly (debug).
2. Help us to find better clissfication.

#### Defination

* $c^{(i)}$: index of cluster ($1, 2, \dots, K$) to which example $x^{(i)}$ is currently assigned.
* $\mu_k$: cluster centroid $k$ ($\mu_k \in R^n$)
* $\mu_{c^{(i)}}$: cluster centroid of cluster to which example $x^{(i)}$ has been assigned.

#### Optimization Objective

$$
J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots, \mu_k) = \frac{1}{m}\sum_{i=1}^m ||x^{(i)} - \mu_{c^{(i)}}||^2
$$

We can find that in the K-means algorithm, we find the $c^{(i)}$ which is the index of cluster centroid closest to $x^{(i)}$, this is equivalent to $\min J()$ , while holding $\mu_i$ fixed.

### Random Initialization

* Should have $K < m$
* Randomly pick $K$ training examples
* Set $\mu_1, \dots, \mu_K$ equal to these $K$ examples.

#### Local optima

Different initial points can lead to different result, may lead to local optima. For example:

![image-20190523223205799](https://markdown-1252040768.cos.ap-beijing.myqcloud.com/2019-05-23-143206.png)

The upper one is great, but the below two are local optimas.

We sould try different initial situation.

We can try many times of initialization, and calculate the cost function.

### Choosing the Number of Clusters

#### Elbow method

![image-20190525095054581](https://markdown-1252040768.cos.ap-beijing.myqcloud.com/2019-05-25-015055.png)

If it’s clearly having an "Elbow", we can choose the albow as the number of $K$, here 3.

## Dimensionality Reduction

### Data Compression

#### Reduce data from 2D to 1D

![image-20190525102352966](https://markdown-1252040768.cos.ap-beijing.myqcloud.com/2019-05-25-022353.png)

By projecting the 2D points to the line, we can get a new 1D feature. We just use one number to repersent the position of each of the training examples.

### Principal Component Analysis (PCA)

Reduce from 2-dimension to 1-dimension: Find a direction (a vector $u^{(1)} \in R^n$) onto which to project the data so as to minimize the projection error.

Reduce from n-dimension to k-dimension: Find $k$ vectors $u^{(1)}, u^{(2)}, \dots, u^{(k)}$ onto which to project the data, so as to minimize the projection error.

![image-20190525112802083](https://markdown-1252040768.cos.ap-beijing.myqcloud.com/2019-05-25-032802.png)

#### Principal Component Analysis Algorithm

##### Data preprocessing

* Training set: $x^{(1)}, x^{(2)}, \dots, x^{(m)}$
* Feature scaling / mean normalization

$$
\mu_j = \frac{1}{m}\sum_{i=1}^mx_j^{(i)}
$$

Replace each $x_j^{(i)}$ with $\frac{x_j - \mu_j}{s_j}$.

##### PCA algorithm

Reduce data from $n$-dimensions to $k$-dimensions

Compute “covariance matrix”
$$
\Sigma=\frac{1}{m}\sum_{i=1}^m(x^{(i)})(x^{(i)})^T
$$
or:
$$
\Sigma = \frac{1}{m}X'\times X
$$
In matlab:

```matlab
Sigma = (1/m) * X' * X
```

Compute “eigenvectors” of matrix $\Sigma$:

```matlab
[U, S, V] = svd(Sigma);
```

* $\Sigma$: $n\times n$ matrix.

* $U$: $n\times n$ matrix, we can see it as $n$ column vectors, and we just take the first $k$ vectors.

![image-20190525164930982](https://markdown-1252040768.cos.ap-beijing.myqcloud.com/2019-05-25-084931.png)

Then we get:

![image-20190525165031905](https://markdown-1252040768.cos.ap-beijing.myqcloud.com/2019-05-25-085032.png)

Then we use it to compute:

![image-20190525165119463](https://markdown-1252040768.cos.ap-beijing.myqcloud.com/2019-05-25-085119.png)

Here $z\in R^k$

##### Reconstruction from compressed representation

$$
z = U_{reduce}^Tx
$$

$$
x_{approx} = U_{reduce}z
$$

#### Choosing the number of principal components

Average squared projection error:
$$
\frac{1}{m}\sum_{i=1}^m||x^{(i)} - x_{approx}^{(i)}||^2
$$

Total variation in the data:
$$
\frac{1}{m}\sum_{i=1}^m||x^{(i)}||^2
$$
Choose $k$ to be smalest value so that
$$
\frac{
\frac{1}{m}\sum_{i=1}^m||x^{(i)} - x_{approx}^{(i)}||^2
}{
\frac{1}{m}\sum_{i=1}^m||x^{(i )}||^2
} \le 0.01
$$
“99% of variance is retained”.

##### Choosing $k$

```matlab
[U, S, V] = svd(Sigma)
```

$$
\frac{
\frac{1}{m}\sum_{i=1}^m||x^{(i)} - x_{approx}^{(i)}||^2
}{
\frac{1}{m}\sum_{i=1}^m||x^{(i )}||^2
} \le 0.01
$$

The left parto of the above equation is equal to:
$$
1 - \frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}}
$$
![image-20190525172053387](https://markdown-1252040768.cos.ap-beijing.myqcloud.com/2019-05-25-092054.png)



